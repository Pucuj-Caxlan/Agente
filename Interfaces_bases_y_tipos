# ====================================================================
# INTERFACES BASE Y TIPOS DE DATOS MEJORADOS
# Con validación robusta, serialización, testing automatizado y utilidades
# ====================================================================

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
from datetime import datetime, timedelta
import json
import uuid
import hashlib
import logging
import asyncio
from pathlib import Path
import unittest
import pytest
from unittest.mock import Mock, patch
import pickle
import numpy as np

# ====================================================================
# 1. ENUMERADORES Y CONSTANTES
# ====================================================================

class AgentState(Enum):
    """Estados del agente con transiciones válidas"""
    INITIALIZING = "initializing"
    ACTIVE = "active"
    LEARNING = "learning"
    IDLE = "idle"
    MAINTENANCE = "maintenance"
    ERROR = "error"
    SHUTDOWN = "shutdown"

    def __str__(self) -> str:
        """Representación legible del estado"""
        return self.value.capitalize()
    
    def can_transition_to(self, target_state: 'AgentState') -> bool:
        """Verifica si puede transicionar a otro estado"""
        valid_transitions = {
            AgentState.INITIALIZING: [AgentState.ACTIVE, AgentState.ERROR],
            AgentState.ACTIVE: [AgentState.LEARNING, AgentState.IDLE, AgentState.MAINTENANCE, AgentState.ERROR, AgentState.SHUTDOWN],
            AgentState.LEARNING: [AgentState.ACTIVE, AgentState.ERROR],
            AgentState.IDLE: [AgentState.ACTIVE, AgentState.MAINTENANCE, AgentState.SHUTDOWN],
            AgentState.MAINTENANCE: [AgentState.ACTIVE, AgentState.ERROR],
            AgentState.ERROR: [AgentState.MAINTENANCE, AgentState.SHUTDOWN, AgentState.INITIALIZING],
            AgentState.SHUTDOWN: []  # Estado terminal
        }
        return target_state in valid_transitions.get(self, [])

class PerceptionType(Enum):
    """Tipos de percepción del agente"""
    SENSOR_READING = "sensor_reading"
    USER_INPUT = "user_input"
    SYSTEM_EVENT = "system_event"
    API_RESPONSE = "api_response"
    DATABASE_RESULT = "database_result"
    FILE_CONTENT = "file_content"
    NETWORK_MESSAGE = "network_message"
    ENVIRONMENTAL = "environmental"

class ActionType(Enum):
    """Tipos de acciones del agente"""
    SENSOR_READ = "sensor_read"
    DATABASE_QUERY = "database_query"
    API_CALL = "api_call"
    FILE_OPERATION = "file_operation"
    COMMUNICATION = "communication"
    COMPUTATION = "computation"
    CONTROL_COMMAND = "control_command"
    LEARNING_UPDATE = "learning_update"

class Priority(Enum):
    """Niveles de prioridad"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    BACKGROUND = 5

# ====================================================================
# 2. EXCEPCIONES PERSONALIZADAS
# ====================================================================

class AgentException(Exception):
    """Excepción base para errores del agente"""
    pass

class ValidationError(AgentException):
    """Error de validación de datos"""
    pass

class StateTransitionError(AgentException):
    """Error en transición de estado"""
    pass

class PerceptionError(AgentException):
    """Error en procesamiento de percepciones"""
    pass

class ActionExecutionError(AgentException):
    """Error en ejecución de acciones"""
    pass

# ====================================================================
# 3. UTILIDADES Y VALIDADORES
# ====================================================================

class DataValidator:
    """Validador de datos para estructuras del agente"""
    
    @staticmethod
    def validate_confidence(value: float, field_name: str = "confidence") -> float:
        """Valida y normaliza valores de confianza"""
        if not isinstance(value, (int, float)):
            raise ValidationError(f"{field_name} debe ser numérico, recibido: {type(value)}")
        
        if not 0.0 <= value <= 1.0:
            raise ValidationError(f"{field_name} debe estar entre 0.0 y 1.0, recibido: {value}")
        
        return float(value)
    
    @staticmethod
    def validate_priority(value: int, field_name: str = "priority") -> int:
        """Valida prioridades"""
        if not isinstance(value, int):
            raise ValidationError(f"{field_name} debe ser entero, recibido: {type(value)}")
        
        if not 1 <= value <= 5:
            raise ValidationError(f"{field_name} debe estar entre 1 y 5, recibido: {value}")
        
        return value
    
    @staticmethod
    def validate_dict_structure(data: Dict, required_fields: List[str], field_name: str = "data") -> Dict:
        """Valida estructura de diccionarios"""
        if not isinstance(data, dict):
            raise ValidationError(f"{field_name} debe ser un diccionario, recibido: {type(data)}")
        
        missing_fields = [field for field in required_fields if field not in data]
        if missing_fields:
            raise ValidationError(f"{field_name} falta campos requeridos: {missing_fields}")
        
        return data
    
    @staticmethod
    def validate_timestamp(timestamp: datetime, field_name: str = "timestamp") -> datetime:
        """Valida timestamps"""
        if not isinstance(timestamp, datetime):
            raise ValidationError(f"{field_name} debe ser datetime, recibido: {type(timestamp)}")
        
        # Verifica que no sea muy antiguo (más de 1 año) o futuro
        now = datetime.now()
        max_age = timedelta(days=365)
        
        if timestamp < (now - max_age):
            raise ValidationError(f"{field_name} es demasiado antiguo: {timestamp}")
        
        if timestamp > (now + timedelta(minutes=5)):  # Permite 5 min de diferencia por sincronización
            raise ValidationError(f"{field_name} está en el futuro: {timestamp}")
        
        return timestamp

class SerializationMixin:
    """Mixin para serialización/deserialización"""
    
    def to_json(self) -> str:
        """Convierte a JSON con manejo de tipos especiales"""
        def json_serializer(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif isinstance(obj, Enum):
                return obj.value
            elif hasattr(obj, '__dict__'):
                return obj.__dict__
            return str(obj)
        
        data = asdict(self) if hasattr(self, '__dataclass_fields__') else self.__dict__
        return json.dumps(data, default=json_serializer, indent=2)
    
    @classmethod
    def from_json(cls, json_str: str):
        """Crea instancia desde JSON"""
        try:
            data = json.loads(json_str)
            return cls.from_dict(data)
        except json.JSONDecodeError as e:
            raise ValidationError(f"JSON inválido: {e}")
    
    @classmethod
    def from_dict(cls, data: Dict):
        """Crea instancia desde diccionario (implementar en subclases)"""
        raise NotImplementedError("Subclases deben implementar from_dict")
    
    def to_bytes(self) -> bytes:
        """Serialización binaria eficiente"""
        return pickle.dumps(self)
    
    @classmethod
    def from_bytes(cls, data: bytes):
        """Deserialización binaria"""
        try:
            return pickle.loads(data)
        except Exception as e:
            raise ValidationError(f"Error deserializando bytes: {e}")

# ====================================================================
# 4. DATACLASS PERCEPTION MEJORADA
# ====================================================================

@dataclass
class Perception(SerializationMixin):
    """
    Estructura para almacenar percepciones del agente.
    
    Attributes:
        source: Identificador único de la fuente (sensor, API, etc.)
        data: Datos de la percepción
        timestamp: Momento de la percepción
        confidence: Nivel de confianza [0.0-1.0]
        metadata: Información adicional sobre la percepción
        perception_id: ID único generado automáticamente
        expires_at: Momento en que la percepción expira (opcional)
    """
    source: str
    data: Dict[str, Any]
    timestamp: datetime
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    perception_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    expires_at: Optional[datetime] = None
    
    def __post_init__(self):
        """Validación y normalización post-inicialización"""
        # Validaciones básicas
        if not isinstance(self.source, str) or not self.source.strip():
            raise ValidationError("Source debe ser una cadena no vacía")
        
        self.source = self.source.strip()
        
        # Valida datos
        if not isinstance(self.data, dict):
            raise ValidationError("Data debe ser un diccionario")
        
        # Valida timestamp
        self.timestamp = DataValidator.validate_timestamp(self.timestamp)
        
        # Valida confianza
        self.confidence = DataValidator.validate_confidence(self.confidence)
        
        # Valida metadata
        if not isinstance(self.metadata, dict):
            raise ValidationError("Metadata debe ser un diccionario")
        
        # Normaliza metadata
        self._normalize_metadata()
        
        # Valida estructura específica según tipo
        self._validate_data_structure()
        
        # Establece expiración por defecto si no se especifica
        if self.expires_at is None:
            self.expires_at = self._calculate_default_expiration()
    
    def _normalize_metadata(self):
        """Normaliza y enriquece metadata"""
        # Asegura campos básicos
        if 'type' not in self.metadata:
            self.metadata['type'] = self._infer_perception_type()
        
        if 'processing_timestamp' not in self.metadata:
            self.metadata['processing_timestamp'] = datetime.now()
        
        if 'hash' not in self.metadata:
            self.metadata['hash'] = self.calculate_hash()
    
    def _infer_perception_type(self) -> str:
        """Infiere el tipo de percepción basado en source y data"""
        source_lower = self.source.lower()
        
        if 'sensor' in source_lower:
            return PerceptionType.SENSOR_READING.value
        elif 'api' in source_lower:
            return PerceptionType.API_RESPONSE.value
        elif 'user' in source_lower:
            return PerceptionType.USER_INPUT.value
        elif 'database' in source_lower or 'db' in source_lower:
            return PerceptionType.DATABASE_RESULT.value
        elif 'file' in source_lower:
            return PerceptionType.FILE_CONTENT.value
        else:
            return PerceptionType.SYSTEM_EVENT.value
    
    def _validate_data_structure(self):
        """Valida estructura de datos según el tipo de percepción"""
        perception_type = self.metadata.get('type')
        
        validation_rules = {
            PerceptionType.SENSOR_READING.value: ['value', 'unit'],
            PerceptionType.API_RESPONSE.value: ['status', 'response'],
            PerceptionType.DATABASE_RESULT.value: ['query', 'results'],
            PerceptionType.FILE_CONTENT.value: ['file_path', 'content']
        }
        
        if perception_type in validation_rules:
            required_fields = validation_rules[perception_type]
            DataValidator.validate_dict_structure(self.data, required_fields, "data")
    
    def _calculate_default_expiration(self) -> datetime:
        """Calcula expiración por defecto según el tipo"""
        perception_type = self.metadata.get('type')
        
        expiration_map = {
            PerceptionType.SENSOR_READING.value: timedelta(minutes=5),
            PerceptionType.API_RESPONSE.value: timedelta(minutes=15),
            PerceptionType.USER_INPUT.value: timedelta(hours=1),
            PerceptionType.DATABASE_RESULT.value: timedelta(minutes=30),
            PerceptionType.FILE_CONTENT.value: timedelta(hours=6)
        }
        
        default_duration = expiration_map.get(perception_type, timedelta(minutes=10))
        return self.timestamp + default_duration
    
    def is_expired(self) -> bool:
        """Verifica si la percepción ha expirado"""
        if self.expires_at is None:
            return False
        return datetime.now() > self.expires_at
    
    def is_stale(self, max_age_seconds: int = 300) -> bool:
        """Verifica si la percepción está obsoleta"""
        age = (datetime.now() - self.timestamp).total_seconds()
        return age > max_age_seconds
    
    def calculate_hash(self) -> str:
        """Calcula hash único de la percepción"""
        content = f"{self.source}_{self.timestamp.isoformat()}_{json.dumps(self.data, sort_keys=True)}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def similarity_score(self, other: 'Perception') -> float:
        """Calcula similitud con otra percepción"""
        if not isinstance(other, Perception):
            return 0.0
        
        # Similitud de fuente
        source_similarity = 1.0 if self.source == other.source else 0.0
        
        # Similitud temporal (más recientes son más similares)
        time_diff = abs((self.timestamp - other.timestamp).total_seconds())
        temporal_similarity = max(0.0, 1.0 - (time_diff / 3600))  # Decae en 1 hora
        
        # Similitud de datos (simplificada)
        data_similarity = self._calculate_data_similarity(other.data)
        
        # Similitud de tipo
        type_similarity = 1.0 if self.metadata.get('type') == other.metadata.get('type') else 0.5
        
        # Promedio ponderado
        weights = [0.3, 0.2, 0.4, 0.1]
        scores = [source_similarity, temporal_similarity, data_similarity, type_similarity]
        
        return sum(w * s for w, s in zip(weights, scores))
    
    def _calculate_data_similarity(self, other_data: Dict) -> float:
        """Calcula similitud entre datos"""
        if not isinstance(other_data, dict):
            return 0.0
        
        # Jaccard similarity para claves
        my_keys = set(self.data.keys())
        other_keys = set(other_data.keys())
        
        if not my_keys and not other_keys:
            return 1.0
        
        intersection = len(my_keys.intersection(other_keys))
        union = len(my_keys.union(other_keys))
        
        return intersection / union if union > 0 else 0.0
    
    def extract_features(self) -> Dict[str, Any]:
        """Extrae características para análisis/ML"""
        return {
            'source_hash': hashlib.md5(self.source.encode()).hexdigest()[:8],
            'confidence': self.confidence,
            'data_size': len(str(self.data)),
            'field_count': len(self.data),
            'perception_type': self.metadata.get('type'),
            'age_seconds': (datetime.now() - self.timestamp).total_seconds(),
            'has_numeric_data': any(isinstance(v, (int, float)) for v in self.data.values()),
            'data_complexity': self._calculate_data_complexity()
        }
    
    def _calculate_data_complexity(self) -> float:
        """Calcula complejidad de los datos"""
        complexity = 0.0
        
        for value in self.data.values():
            if isinstance(value, dict):
                complexity += 2.0
            elif isinstance(value, list):
                complexity += 1.5
            elif isinstance(value, str) and len(value) > 100:
                complexity += 1.0
            else:
                complexity += 0.5
        
        return complexity
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'Perception':
        """Crea Perception desde diccionario"""
        # Maneja conversión de timestamp
        if isinstance(data.get('timestamp'), str):
            data['timestamp'] = datetime.fromisoformat(data['timestamp'])
        
        # Maneja conversión de expires_at
        if isinstance(data.get('expires_at'), str):
            data['expires_at'] = datetime.fromisoformat(data['expires_at'])
        
        return cls(**data)
    
    def __str__(self) -> str:
        """Representación legible"""
        return f"Perception(source={self.source}, type={self.metadata.get('type')}, confidence={self.confidence:.2f})"
    
    def __repr__(self) -> str:
        """Representación detallada"""
        return f"Perception(id={self.perception_id[:8]}, source={self.source}, confidence={self.confidence})"

# ====================================================================
# 5. DATACLASS ACTION MEJORADA
# ====================================================================

@dataclass
class Action(SerializationMixin):
    """
    Estructura para representar acciones del agente.
    
    Attributes:
        action_type: Tipo de acción a ejecutar
        parameters: Parámetros específicos de la acción
        priority: Prioridad de ejecución [1-5]
        expected_outcome: Resultado esperado (opcional)
        execution_time: Momento programado de ejecución (opcional)
        action_id: ID único generado automáticamente
        timeout: Tiempo máximo de ejecución
        prerequisites: Condiciones previas requeridas
        resources_required: Recursos necesarios
    """
    action_type: str
    parameters: Dict[str, Any]
    priority: int
    expected_outcome: Optional[str] = None
    execution_time: Optional[datetime] = None
    action_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timeout: timedelta = field(default_factory=lambda: timedelta(minutes=5))
    prerequisites: List[str] = field(default_factory=list)
    resources_required: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validación y normalización post-inicialización"""
        # Valida action_type
        if not isinstance(self.action_type, str) or not self.action_type.strip():
            raise ValidationError("action_type debe ser una cadena no vacía")
        
        self.action_type = self.action_type.strip().lower()
        
        # Valida parameters
        if not isinstance(self.parameters, dict):
            raise ValidationError("parameters debe ser un diccionario")
        
        # Valida prioridad
        self.priority = DataValidator.validate_priority(self.priority)
        
        # Valida execution_time si se proporciona
        if self.execution_time is not None:
            self.execution_time = DataValidator.validate_timestamp(self.execution_time, "execution_time")
        
        # Valida timeout
        if not isinstance(self.timeout, timedelta):
            raise ValidationError("timeout debe ser un timedelta")
        
        if self.timeout.total_seconds() <= 0:
            raise ValidationError("timeout debe ser positivo")
        
        # Normaliza metadata
        self._normalize_metadata()
        
        # Calcula estimaciones automáticas
        self._calculate_estimates()
    
    def _normalize_metadata(self):
        """Normaliza y enriquece metadata"""
        if 'created_at' not in self.metadata:
            self.metadata['created_at'] = datetime.now()
        
        if 'category' not in self.metadata:
            self.metadata['category'] = self._categorize_action()
        
        if 'hash' not in self.metadata:
            self.metadata['hash'] = self.calculate_hash()
    
    def _categorize_action(self) -> str:
        """Categoriza la acción automáticamente"""
        type_lower = self.action_type.lower()
        
        if any(keyword in type_lower for keyword in ['read', 'sensor', 'monitor']):
            return 'perception'
        elif any(keyword in type_lower for keyword in ['write', 'update', 'create', 'delete']):
            return 'modification'
        elif any(keyword in type_lower for keyword in ['send', 'message', 'notify']):
            return 'communication'
        elif any(keyword in type_lower for keyword in ['compute', 'calculate', 'analyze']):
            return 'computation'
        else:
            return 'general'
    
    def _calculate_estimates(self):
        """Calcula estimaciones automáticas"""
        # Estimación de duración basada en tipo y parámetros
        if 'estimated_duration' not in self.metadata:
            self.metadata['estimated_duration'] = self.estimate_duration()
        
        # Estimación de recursos si no se especificaron
        if not self.resources_required:
            self.resources_required = self.estimate_resource_requirements()
    
    def estimate_duration(self) -> timedelta:
        """Estima duración de ejecución"""
        duration_map = {
            'sensor_read': timedelta(seconds=1),
            'database_query': timedelta(seconds=5),
            'api_call': timedelta(seconds=10),
            'file_operation': timedelta(seconds=30),
            'computation': timedelta(minutes=1),
            'communication': timedelta(seconds=5),
            'learning_update': timedelta(minutes=2)
        }
        
        base_duration = duration_map.get(self.action_type, timedelta(seconds=30))
        
        # Ajusta según complejidad de parámetros
        complexity_factor = 1.0 + (len(self.parameters) * 0.1)
        
        # Ajusta según prioridad (menor prioridad = estimación más conservadora)
        priority_factor = 1.0 + (self.priority - 1) * 0.2
        
        total_seconds = base_duration.total_seconds() * complexity_factor * priority_factor
        return timedelta(seconds=total_seconds)
    
    def estimate_resource_requirements(self) -> Dict[str, Any]:
        """Estima recursos necesarios"""
        base_requirements = {
            'cpu_intensive': False,
            'memory_mb': 100,
            'network_required': False,
            'disk_space_mb': 0,
            'exclusive_access': False
        }
        
        # Ajustes específicos por tipo
        type_adjustments = {
            'computation': {'cpu_intensive': True, 'memory_mb': 500},
            'api_call': {'network_required': True},
            'file_operation': {'disk_space_mb': 100},
            'database_query': {'memory_mb': 200, 'network_required': True},
            'learning_update': {'cpu_intensive': True, 'memory_mb': 1000}
        }
        
        if self.action_type in type_adjustments:
            base_requirements.update(type_adjustments[self.action_type])
        
        return base_requirements
    
    def is_due(self) -> bool:
        """Verifica si la acción debe ejecutarse ahora"""
        if self.execution_time is None:
            return True  # Sin tiempo programado = ejecutar inmediatamente
        
        return datetime.now() >= self.execution_time
    
    def is_expired(self) -> bool:
        """Verifica si la acción ha expirado"""
        if self.execution_time is None:
            return False
        
        return datetime.now() > (self.execution_time + self.timeout)
    
    def can_execute_with_resources(self, available_resources: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """
        Verifica si puede ejecutarse con los recursos disponibles
        
        Returns:
            Tuple[bool, List[str]]: (puede_ejecutar, razones_de_fallo)
        """
        can_execute = True
        reasons = []
        
        # Verifica CPU
        if (self.resources_required.get('cpu_intensive', False) and 
            available_resources.get('cpu_usage', 0) > 80):
            can_execute = False
            reasons.append("CPU usage too high")
        
        # Verifica memoria
        required_memory = self.resources_required.get('memory_mb', 0)
        available_memory = available_resources.get('available_memory_mb', float('inf'))
        if required_memory > available_memory:
            can_execute = False
            reasons.append(f"Insufficient memory: need {required_memory}MB, have {available_memory}MB")
        
        # Verifica red
        if (self.resources_required.get('network_required', False) and 
            not available_resources.get('network_available', True)):
            can_execute = False
            reasons.append("Network not available")
        
        # Verifica espacio en disco
        required_disk = self.resources_required.get('disk_space_mb', 0)
        available_disk = available_resources.get('available_disk_mb', float('inf'))
        if required_disk > available_disk:
            can_execute = False
            reasons.append(f"Insufficient disk space: need {required_disk}MB, have {available_disk}MB")
        
        return can_execute, reasons
    
    def calculate_hash(self) -> str:
        """Calcula hash único de la acción"""
        content = f"{self.action_type}_{json.dumps(self.parameters, sort_keys=True)}_{self.priority}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def create_execution_plan(self) -> Dict[str, Any]:
        """Crea plan de ejecución detallado"""
        return {
            'action_id': self.action_id,
            'steps': self._decompose_into_steps(),
            'resource_allocation': self.resources_required,
            'estimated_duration': self.metadata.get('estimated_duration'),
            'rollback_plan': self._create_rollback_plan(),
            'monitoring_points': self._define_monitoring_points()
        }
    
    def _decompose_into_steps(self) -> List[Dict[str, Any]]:
        """Descompone la acción en pasos ejecutables"""
        # Implementación básica - puede ser extendida por subclases
        return [
            {
                'step_id': 1,
                'description': f"Execute {self.action_type}",
                'parameters': self.parameters,
                'estimated_duration': self.metadata.get('estimated_duration')
            }
        ]
    
    def _create_rollback_plan(self) -> Dict[str, Any]:
        """Crea plan de rollback en caso de fallo"""
        return {
            'rollback_possible': True,
            'rollback_steps': [
                {
                    'step': "Log failure",
                    'action': "log_execution_failure"
                },
                {
                    'step': "Restore state",
                    'action': "restore_previous_state"
                }
            ]
        }
    
    def _define_monitoring_points(self) -> List[Dict[str, Any]]:
        """Define puntos de monitoreo durante la ejecución"""
        return [
            {
                'point': 'start',
                'metrics': ['start_time', 'resource_usage']
            },
            {
                'point': 'progress_50',
                'metrics': ['elapsed_time', 'memory_usage']
            },
            {
                'point': 'completion',
                'metrics': ['end_time', 'success_status', 'resource_usage']
            }
        ]
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'Action':
        """Crea Action desde diccionario"""
        # Maneja conversión de execution_time
        if isinstance(data.get('execution_time'), str):
            data['execution_time'] = datetime.fromisoformat(data['execution_time'])
        
        # Maneja conversión de timeout
        if isinstance(data.get('timeout'), (int, float)):
            data['timeout'] = timedelta(seconds=data['timeout'])
        elif isinstance(data.get('timeout'), str):
            # Asume formato "HH:MM:SS"
            time_parts = data['timeout'].split(':')
            hours, minutes, seconds = map(int, time_parts)
            data['timeout'] = timedelta(hours=hours, minutes=minutes, seconds=seconds)
        
        return cls(**data)
    
    def __str__(self) -> str:
        """Representación legible"""
        return f"Action(type={self.action_type}, priority={self.priority}, due={self.is_due()})"
    
    def __repr__(self) -> str:
        """Representación detallada"""
        return f"Action(id={self.action_id[:8]}, type={self.action_type}, priority={self.priority})"

# ====================================================================
# 6. DATACLASS DECISION MEJORADA
# ====================================================================

@dataclass
class Decision(SerializationMixin):
    """
    Estructura para representar decisiones del agente.
    
    Attributes:
        decision_id: ID único de la decisión
        context: Contexto en el que se tomó la decisión
        options: Opciones consideradas
        selected_action: Acción seleccionada
        reasoning: Explicación del razonamiento
        confidence: Nivel de confianza en la decisión
        decision_time: Momento de la decisión
        execution_status: Estado de ejecución
        outcome: Resultado de la decisión (posterior)
    """
    decision_id: str
    context: Dict[str, Any]
    options: List[Action]
    selected_action: Action
    reasoning: str
    confidence: float
    decision_time: datetime = field(default_factory=datetime.now)
    execution_status: str = "pending"
    outcome: Optional[Dict[str, Any]] = None
    alternative_rankings: List[Tuple[Action, float]] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validación y normalización post-inicialización"""
        # Valida decision_id
        if not isinstance(self.decision_id, str) or not self.decision_id.strip():
            raise ValidationError("decision_id debe ser una cadena no vacía")
        
        # Valida context
        if not isinstance(self.context, dict):
            raise ValidationError("context debe ser un diccionario")
        
        # Valida options
        if not isinstance(self.options, list):
            raise ValidationError("options debe ser una lista")
        
        if not self.options:
            raise ValidationError("options no puede estar vacía")
        
        for i, option in enumerate(self.options):
            if not isinstance(option, Action):
                raise ValidationError(f"options[{i}] debe ser una instancia de Action")
        
        # Valida selected_action
        if not isinstance(self.selected_action, Action):
            raise ValidationError("selected_action debe ser una instancia de Action")
        
        if self.selected_action not in self.options:
            raise ValidationError("selected_action debe estar en options")
        
        # Valida reasoning
        if not isinstance(self.reasoning, str) or not self.reasoning.strip():
            raise ValidationError("reasoning debe ser una cadena no vacía")
        
        # Valida confianza
        self.confidence = DataValidator.validate_confidence(self.confidence)
        
        # Valida decision_time
        self.decision_time = DataValidator.validate_timestamp(self.decision_time, "decision_time")
        
        # Valida execution_status
        valid_statuses = ["pending", "executing", "completed", "failed", "cancelled"]
        if self.execution_status not in valid_statuses:
            raise ValidationError(f"execution_status debe ser uno de: {valid_statuses}")
        
        # Normaliza metadata
        self._normalize_metadata()
        
        # Calcula rankings si no se proporcionaron
        if not self.alternative_rankings:
            self.alternative_rankings = self._calculate_alternative_rankings()
    
    def _normalize_metadata(self):
        """Normaliza y enriquece metadata"""
        if 'created_at' not in self.metadata:
            self.metadata['created_at'] = datetime.now()
        
        if 'decision_method' not in self.metadata:
            self.metadata['decision_method'] = 'utility_maximization'
        
        if 'context_complexity' not in self.metadata:
            self.metadata['context_complexity'] = self._calculate_context_complexity()
        
        if 'options_count' not in self.metadata:
            self.metadata['options_count'] = len(self.options)
        
        if 'hash' not in self.metadata:
            self.metadata['hash'] = self.calculate_hash()
    
    def _calculate_context_complexity(self) -> float:
        """Calcula complejidad del contexto de decisión"""
        complexity = 0.0
        
        # Complejidad basada en número de elementos en el contexto
        complexity += len(self.context) * 0.1
        
        # Complejidad basada en profundidad de anidamiento
        for value in self.context.values():
            if isinstance(value, dict):
                complexity += 0.5
            elif isinstance(value, list):
                complexity += 0.3
        
        # Complejidad basada en número de opciones
        complexity += len(self.options) * 0.2
        
        return complexity
    
    def _calculate_alternative_rankings(self) -> List[Tuple[Action, float]]:
        """Calcula rankings de alternativas"""
        # Implementación simplificada - en realidad usaría el motor de decisión
        rankings = []
        
        for action in self.options:
            if action == self.selected_action:
                score = self.confidence
            else:
                # Score basado en prioridad y estimación simple
                score = max(0.1, 1.0 - (action.priority * 0.1))
                if action.action_type == self.selected_action.action_type:
                    score += 0.2  # Bonus por tipo similar
            
            rankings.append((action, score))
        
        return sorted(rankings, key=lambda x: x[1], reverse=True)
    
    def update_execution_status(self, new_status: str, outcome: Optional[Dict[str, Any]] = None):
        """Actualiza estado de ejecución y resultado"""
        valid_statuses = ["pending", "executing", "completed", "failed", "cancelled"]
        if new_status not in valid_statuses:
            raise ValidationError(f"Invalid execution status: {new_status}")
        
        old_status = self.execution_status
        self.execution_status = new_status
        
        if outcome is not None:
            self.outcome = outcome
        
        # Registra transición en metadata
        if 'status_history' not in self.metadata:
            self.metadata['status_history'] = []
        
        self.metadata['status_history'].append({
            'from_status': old_status,
            'to_status': new_status,
            'timestamp': datetime.now(),
            'outcome_provided': outcome is not None
        })
    
    def calculate_decision_quality(self) -> Dict[str, float]:
        """Calcula métricas de calidad de la decisión"""
        if self.outcome is None:
            return {'quality_score': None, 'reason': 'No outcome available yet'}
        
        # Métricas básicas de calidad
        success_score = 1.0 if self.outcome.get('success', False) else 0.0
        efficiency_score = self.outcome.get('efficiency_score', 0.5)
        timeliness_score = self._calculate_timeliness_score()
        
        # Score de calidad combinado
        quality_score = (
            0.4 * success_score +
            0.3 * efficiency_score +
            0.3 * timeliness_score
        )
        
        return {
            'overall_quality': quality_score,
            'success_score': success_score,
            'efficiency_score': efficiency_score,
            'timeliness_score': timeliness_score,
            'confidence_calibration': self._calculate_confidence_calibration()
        }
    
    def _calculate_timeliness_score(self) -> float:
        """Calcula score de puntualidad"""
        if 'execution_start' not in self.metadata or 'execution_end' not in self.metadata:
            return 0.5  # Score neutral si no hay datos de timing
        
        start_time = self.metadata['execution_start']
        end_time = self.metadata['execution_end']
        actual_duration = (end_time - start_time).total_seconds()
        
        estimated_duration = self.selected_action.metadata.get('estimated_duration')
        if estimated_duration is None:
            return 0.5
        
        estimated_seconds = estimated_duration.total_seconds()
        
        # Score basado en qué tan cerca estuvo la estimación
        if estimated_seconds == 0:
            return 0.5
        
        ratio = actual_duration / estimated_seconds
        
        # Score óptimo cuando ratio está entre 0.8 y 1.2
        if 0.8 <= ratio <= 1.2:
            return 1.0
        elif 0.5 <= ratio <= 2.0:
            return max(0.5, 1.0 - abs(ratio - 1.0))
        else:
            return 0.1
    
    def _calculate_confidence_calibration(self) -> float:
        """Calcula qué tan bien calibrada estaba la confianza"""
        if self.outcome is None:
            return 0.5
        
        actual_success = 1.0 if self.outcome.get('success', False) else 0.0
        predicted_confidence = self.confidence
        
        # Calibración perfecta cuando confianza = éxito actual
        calibration_error = abs(predicted_confidence - actual_success)
        calibration_score = max(0.0, 1.0 - calibration_error)
        
        return calibration_score
    
    def generate_explanation(self) -> Dict[str, Any]:
        """Genera explicación detallada de la decisión"""
        explanation = {
            'decision_summary': f"Selected '{self.selected_action.action_type}' with {self.confidence:.1%} confidence",
            'reasoning': self.reasoning,
            'context_factors': self._extract_key_context_factors(),
            'alternatives_considered': len(self.options),
            'selection_criteria': self._extract_selection_criteria(),
            'risk_assessment': self._assess_decision_risks(),
            'expected_vs_actual': self._compare_expected_vs_actual()
        }
        
        return explanation
    
    def _extract_key_context_factors(self) -> List[str]:
        """Extrae factores clave del contexto"""
        key_factors = []
        
        # Busca factores importantes en el contexto
        important_keys = ['urgency', 'resources', 'constraints', 'objectives', 'risks']
        
        for key in important_keys:
            if key in self.context:
                key_factors.append(f"{key}: {self.context[key]}")
        
        return key_factors
    
    def _extract_selection_criteria(self) -> List[str]:
        """Extrae criterios de selección de la decisión"""
        criteria = []
        
        # Analiza la acción seleccionada vs alternativas
        selected = self.selected_action
        
        # Criterio de prioridad
        if all(selected.priority <= action.priority for action in self.options):
            criteria.append("Highest priority action")
        
        # Criterio de confianza
        selected_score = next((score for action, score in self.alternative_rankings 
                             if action == selected), 0)
        if selected_score == max(score for _, score in self.alternative_rankings):
            criteria.append("Highest utility score")
        
        return criteria
    
    def _assess_decision_risks(self) -> Dict[str, Any]:
        """Evalúa riesgos de la decisión"""
        return {
            'confidence_risk': 'High' if self.confidence < 0.6 else 'Low',
            'complexity_risk': 'High' if self.metadata.get('context_complexity', 0) > 2.0 else 'Low',
            'time_pressure': 'High' if self.context.get('urgency', 'normal') == 'high' else 'Low',
            'resource_risk': 'High' if self.selected_action.resources_required.get('cpu_intensive') else 'Low'
        }
    
    def _compare_expected_vs_actual(self) -> Optional[Dict[str, Any]]:
        """Compara resultados esperados vs actuales"""
        if self.outcome is None:
            return None
        
        return {
            'expected_outcome': self.selected_action.expected_outcome,
            'actual_outcome': self.outcome.get('description'),
            'expectation_met': self.outcome.get('success', False),
            'variance_analysis': self.outcome.get('variance_from_expected')
        }
    
    def calculate_hash(self) -> str:
        """Calcula hash único de la decisión"""
        # Hash basado en elementos inmutables de la decisión
        action_hashes = [action.calculate_hash() for action in self.options]
        content = f"{self.decision_id}_{self.decision_time.isoformat()}_{self.selected_action.calculate_hash()}_{sorted(action_hashes)}"
        return hashlib.md5(content.encode()).hexdigest()
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'Decision':
        """Crea Decision desde diccionario"""
        # Convierte actions de dict a objetos Action
        if 'options' in data and isinstance(data['options'][0], dict):
            data['options'] = [Action.from_dict(action_dict) for action_dict in data['options']]
        
        if 'selected_action' in data and isinstance(data['selected_action'], dict):
            data['selected_action'] = Action.from_dict(data['selected_action'])
        
        # Maneja conversión de timestamps
        if isinstance(data.get('decision_time'), str):
            data['decision_time'] = datetime.fromisoformat(data['decision_time'])
        
        # Reconstruye alternative_rankings si está serializado
        if 'alternative_rankings' in data and data['alternative_rankings']:
            if isinstance(data['alternative_rankings'][0][0], dict):
                data['alternative_rankings'] = [
                    (Action.from_dict(action_dict), score) 
                    for action_dict, score in data['alternative_rankings']
                ]
        
        return cls(**data)
    
    def __str__(self) -> str:
        """Representación legible"""
        return f"Decision(id={self.decision_id[:8]}, action={self.selected_action.action_type}, confidence={self.confidence:.1%})"
    
    def __repr__(self) -> str:
        """Representación detallada"""
        return f"Decision(id={self.decision_id}, options={len(self.options)}, status={self.execution_status})"

# ====================================================================
# 7. SISTEMA DE TESTING AUTOMATIZADO
# ====================================================================

class TestDataFactory:
    """Factory para crear datos de prueba"""
    
    @staticmethod
    def create_sample_perception(
        source: str = "test_sensor",
        confidence: float = 0.8,
        perception_type: str = PerceptionType.SENSOR_READING.value
    ) -> Perception:
        """Crea percepción de prueba"""
        data = {
            'value': 25.5,
            'unit': 'celsius',
            'sensor_id': 'temp_001'
        }
        
        metadata = {
            'type': perception_type,
            'location': 'test_site',
            'calibration_date': '2024-01-01'
        }
        
        return Perception(
            source=source,
            data=data,
            timestamp=datetime.now(),
            confidence=confidence,
            metadata=metadata
        )
    
    @staticmethod
    def create_sample_action(
        action_type: str = "sensor_read",
        priority: int = 3
    ) -> Action:
        """Crea acción de prueba"""
        parameters = {
            'sensor_id': 'temp_001',
            'timeout': 30,
            'retry_count': 3
        }
        
        return Action(
            action_type=action_type,
            parameters=parameters,
            priority=priority,
            expected_outcome="Temperature reading obtained"
        )
    
    @staticmethod
    def create_sample_decision(
        options_count: int = 3,
        confidence: float = 0.75
    ) -> Decision:
        """Crea decisión de prueba"""
        options = [
            TestDataFactory.create_sample_action(f"action_{i}", i+1) 
            for i in range(options_count)
        ]
        
        selected_action = options[0]  # Selecciona primera opción
        
        context = {
            'urgency': 'normal',
            'available_resources': {'cpu': 50, 'memory': 1024},
            'constraints': ['budget_limit', 'time_limit']
        }
        
        return Decision(
            decision_id=str(uuid.uuid4()),
            context=context,
            options=options,
            selected_action=selected_action,
            reasoning="Selected based on highest priority and resource availability",
            confidence=confidence
        )

class TestAgentBase(unittest.TestCase):
    """Clase base para tests de agentes"""
    
    def setUp(self):
        """Configuración común para tests"""
        self.test_factory = TestDataFactory()
        logging.basicConfig(level=logging.DEBUG)
    
    def tearDown(self):
        """Limpieza después de tests"""
        pass
    
    def assert_valid_perception(self, perception: Perception):
        """Valida que una percepción sea correcta"""
        self.assertIsInstance(perception, Perception)
        self.assertIsInstance(perception.source, str)
        self.assertTrue(perception.source.strip())
        self.assertIsInstance(perception.data, dict)
        self.assertIsInstance(perception.timestamp, datetime)
        self.assertTrue(0.0 <= perception.confidence <= 1.0)
        self.assertIsInstance(perception.metadata, dict)
        self.assertIsInstance(perception.perception_id, str)
    
    def assert_valid_action(self, action: Action):
        """Valida que una acción sea correcta"""
        self.assertIsInstance(action, Action)
        self.assertIsInstance(action.action_type, str)
        self.assertTrue(action.action_type.strip())
        self.assertIsInstance(action.parameters, dict)
        self.assertTrue(1 <= action.priority <= 5)
        self.assertIsInstance(action.action_id, str)
        self.assertIsInstance(action.timeout, timedelta)
        self.assertTrue(action.timeout.total_seconds() > 0)
    
    def assert_valid_decision(self, decision: Decision):
        """Valida que una decisión sea correcta"""
        self.assertIsInstance(decision, Decision)
        self.assertIsInstance(decision.decision_id, str)
        self.assertIsInstance(decision.context, dict)
        self.assertIsInstance(decision.options, list)
        self.assertTrue(len(decision.options) > 0)
        self.assertIsInstance(decision.selected_action, Action)
        self.assertIn(decision.selected_action, decision.options)
        self.assertIsInstance(decision.reasoning, str)
        self.assertTrue(0.0 <= decision.confidence <= 1.0)

# ====================================================================
# 8. TESTS ESPECÍFICOS
# ====================================================================

class TestPerception(TestAgentBase):
    """Tests para la clase Perception"""
    
    def test_perception_creation(self):
        """Test creación básica de percepción"""
        perception = self.test_factory.create_sample_perception()
        self.assert_valid_perception(perception)
    
    def test_perception_validation(self):
        """Test validación de datos"""
        # Test source vacío
        with self.assertRaises(ValidationError):
            Perception(
                source="",
                data={},
                timestamp=datetime.now(),
                confidence=0.5
            )
        
        # Test confidence fuera de rango
        with self.assertRaises(ValidationError):
            Perception(
                source="test",
                data={},
                timestamp=datetime.now(),
                confidence=1.5
            )
    
    def test_perception_serialization(self):
        """Test serialización/deserialización"""
        original = self.test_factory.create_sample_perception()
        
        # Test JSON
        json_str = original.to_json()
        self.assertIsInstance(json_str, str)
        
        reconstructed = Perception.from_json(json_str)
        self.assertEqual(original.source, reconstructed.source)
        self.assertEqual(original.confidence, reconstructed.confidence)
        
        # Test bytes
        bytes_data = original.to_bytes()
        reconstructed_bytes = Perception.from_bytes(bytes_data)
        self.assertEqual(original.source, reconstructed_bytes.source)
    
    def test_perception_expiration(self):
        """Test lógica de expiración"""
        perception = self.test_factory.create_sample_perception()
        
        # No debe estar expirada inmediatamente
        self.assertFalse(perception.is_expired())
        
        # Test con expiración manual
        perception.expires_at = datetime.now() - timedelta(minutes=1)
        self.assertTrue(perception.is_expired())
    
    def test_perception_similarity(self):
        """Test cálculo de similitud"""
        perception1 = self.test_factory.create_sample_perception("sensor_1")
        perception2 = self.test_factory.create_sample_perception("sensor_1")
        perception3 = self.test_factory.create_sample_perception("sensor_2")
        
        # Misma fuente debe tener alta similitud
        similarity_same = perception1.similarity_score(perception2)
        similarity_different = perception1.similarity_score(perception3)
        
        self.assertTrue(similarity_same > similarity_different)
        self.assertTrue(0.0 <= similarity_same <= 1.0)
        self.assertTrue(0.0 <= similarity_different <= 1.0)

class TestAction(TestAgentBase):
    """Tests para la clase Action"""
    
    def test_action_creation(self):
        """Test creación básica de acción"""
        action = self.test_factory.create_sample_action()
        self.assert_valid_action(action)
    
    def test_action_validation(self):
        """Test validación de acciones"""
        # Test prioridad inválida
        with self.assertRaises(ValidationError):
            Action(
                action_type="test",
                parameters={},
                priority=10  # Fuera de rango
            )
        
        # Test timeout negativo
        with self.assertRaises(ValidationError):
            Action(
                action_type="test",
                parameters={},
                priority=1,
                timeout=timedelta(seconds=-1)
            )
    
    def test_action_resource_estimation(self):
        """Test estimación de recursos"""
        action = Action(
            action_type="computation",
            parameters={},
            priority=1
        )
        
        resources = action.estimate_resource_requirements()
        self.assertIsInstance(resources, dict)
        self.assertTrue(resources.get('cpu_intensive', False))
    
    def test_action_execution_planning(self):
        """Test planificación de ejecución"""
        action = self.test_factory.create_sample_action()
        plan = action.create_execution_plan()
        
        self.assertIsInstance(plan, dict)
        self.assertIn('action_id', plan)
        self.assertIn('steps', plan)
        self.assertIn('resource_allocation', plan)
    
    def test_action_resource_checking(self):
        """Test verificación de recursos"""
        action = self.test_factory.create_sample_action()
        
        # Recursos suficientes
        available_resources = {
            'cpu_usage': 20,
            'available_memory_mb': 2048,
            'network_available': True,
            'available_disk_mb': 1000
        }
        
        can_execute, reasons = action.can_execute_with_resources(available_resources)
        self.assertTrue(can_execute)
        self.assertEqual(len(reasons), 0)
        
        # Recursos insuficientes
        limited_resources = {
            'cpu_usage': 95,  # Alto uso de CPU
            'available_memory_mb': 50,  # Poca memoria
            'network_available': False,
            'available_disk_mb': 10
        }
        
        can_execute, reasons = action.can_execute_with_resources(limited_resources)
        # Puede fallar dependiendo de los requerimientos específicos
        self.assertIsInstance(reasons, list)

class TestDecision(TestAgentBase):
    """Tests para la clase Decision"""
    
    def test_decision_creation(self):
        """Test creación básica de decisión"""
        decision = self.test_factory.create_sample_decision()
        self.assert_valid_decision(decision)
    
    def test_decision_validation(self):
        """Test validación de decisiones"""
        action1 = self.test_factory.create_sample_action()
        action2 = self.test_factory.create_sample_action("different_action")
        
        # Test acción seleccionada no en opciones
        with self.assertRaises(ValidationError):
            Decision(
                decision_id="test_decision",
                context={},
                options=[action1],
                selected_action=action2,  # No está en options
                reasoning="Test",
                confidence=0.5
            )
    
    def test_decision_status_updates(self):
        """Test actualizaciones de estado"""
        decision = self.test_factory.create_sample_decision()
        
        # Estado inicial
        self.assertEqual(decision.execution_status, "pending")
        
        # Actualizar estado
        decision.update_execution_status("executing")
        self.assertEqual(decision.execution_status, "executing")
        
        # Verificar historial
        self.assertIn('status_history', decision.metadata)
        self.assertTrue(len(decision.metadata['status_history']) > 0)
    
    def test_decision_quality_calculation(self):
        """Test cálculo de calidad de decisión"""
        decision = self.test_factory.create_sample_decision()
        
        # Sin outcome
        quality = decision.calculate_decision_quality()
        self.assertIsNone(quality['quality_score'])
        
        # Con outcome exitoso
        decision.outcome = {
            'success': True,
            'efficiency_score': 0.8,
            'description': 'Task completed successfully'
        }
        
        quality = decision.calculate_decision_quality()
        self.assertIsNotNone(quality['quality_score'])
        self.assertTrue(0.0 <= quality['quality_score'] <= 1.0)
    
    def test_decision_explanation(self):
        """Test generación de explicaciones"""
        decision = self.test_factory.create_sample_decision()
        explanation = decision.generate_explanation()
        
        self.assertIsInstance(explanation, dict)
        self.assertIn('decision_summary', explanation)
        self.assertIn('reasoning', explanation)
        self.assertIn('alternatives_considered', explanation)

class TestAgentStateTransitions(TestAgentBase):
    """Tests para transiciones de estado del agente"""
    
    def test_valid_state_transitions(self):
        """Test transiciones válidas de estado"""
        # INITIALIZING -> ACTIVE
        self.assertTrue(AgentState.INITIALIZING.can_transition_to(AgentState.ACTIVE))
        
        # ACTIVE -> LEARNING
        self.assertTrue(AgentState.ACTIVE.can_transition_to(AgentState.LEARNING))
        
        # SHUTDOWN (estado terminal)
        self.assertFalse(AgentState.SHUTDOWN.can_transition_to(AgentState.ACTIVE))
    
    def test_invalid_state_transitions(self):
        """Test transiciones inválidas"""
        # LEARNING -> IDLE (no permitida directamente)
        self.assertFalse(AgentState.LEARNING.can_transition_to(AgentState.IDLE))

# ====================================================================
# 9. TESTS DE INTEGRACIÓN
# ====================================================================

class TestIntegration(TestAgentBase):
    """Tests de integración entre componentes"""
    
    def test_perception_to_action_workflow(self):
        """Test flujo de percepción a acción"""
        # Crear percepción
        perception = self.test_factory.create_sample_perception()
        
        # Simular que la percepción genera necesidad de acción
        if perception.data.get('value', 0) > 20:
            action = Action(
                action_type="temperature_adjustment",
                parameters={'target_temp': 20, 'current_temp': perception.data['value']},
                priority=2
            )
            
            self.assert_valid_action(action)
    
    def test_action_to_decision_workflow(self):
        """Test flujo de acción a decisión"""
        # Crear múltiples acciones
        actions = [
            self.test_factory.create_sample_action("action_1", 1),
            self.test_factory.create_sample_action("action_2", 2),
            self.test_factory.create_sample_action("action_3", 3)
        ]
        
        # Crear decisión basada en acciones
        decision = Decision(
            decision_id=str(uuid.uuid4()),
            context={'test': True},
            options=actions,
            selected_action=actions[0],  # Seleccionar por prioridad
            reasoning="Selected highest priority action",
            confidence=0.8
        )
        
        self.assert_valid_decision(decision)
    
    def test_complete_perception_decision_cycle(self):
        """Test ciclo completo de percepción a decisión"""
        # 1. Crear percepción
        perception = self.test_factory.create_sample_perception()
        
        # 2. Generar acciones basadas en percepción
        actions = []
        if perception.confidence > 0.7:
            actions.append(Action(
                action_type="high_confidence_action",
                parameters={'perception_id': perception.perception_id},
                priority=1
            ))
        else:
            actions.append(Action(
                action_type="low_confidence_action",
                parameters={'perception_id': perception.perception_id},
                priority=3
            ))
        
        # Acción adicional siempre disponible
        actions.append(Action(
            action_type="wait",
            parameters={},
            priority=5
        ))
        
        # 3. Crear decisión
        decision = Decision(
            decision_id=str(uuid.uuid4()),
            context={
                'perception': perception.perception_id,
                'confidence_threshold': 0.7
            },
            options=actions,
            selected_action=actions[0],
            reasoning=f"Selected based on perception confidence: {perception.confidence}",
            confidence=perception.confidence
        )
        
        # 4. Validar todo el flujo
        self.assert_valid_perception(perception)
        for action in actions:
            self.assert_valid_action(action)
        self.assert_valid_decision(decision)
        
        # 5. Verificar coherencia
        self.assertEqual(
            decision.context['perception'],
            perception.perception_id
        )

# ====================================================================
# 10. TESTS DE PERFORMANCE
# ====================================================================

class TestPerformance(TestAgentBase):
    """Tests de rendimiento"""
    
    def test_perception_creation_performance(self):
        """Test rendimiento de creación de percepciones"""
        import time
        
        start_time = time.time()
        perceptions = []
        
        for i in range(1000):
            perception = self.test_factory.create_sample_perception(f"sensor_{i}")
            perceptions.append(perception)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Debe crear 1000 percepciones en menos de 1 segundo
        self.assertLess(duration, 1.0, "Perception creation too slow")
        self.assertEqual(len(perceptions), 1000)
    
    def test_serialization_performance(self):
        """Test rendimiento de serialización"""
        import time
        
        perceptions = [
            self.test_factory.create_sample_perception(f"sensor_{i}")
            for i in range(100)
        ]
        
        # Test JSON serialization
        start_time = time.time()
        for perception in perceptions:
            json_str = perception.to_json()
            reconstructed = Perception.from_json(json_str)
        end_time = time.time()
        
        json_duration = end_time - start_time
        
        # Test binary serialization
        start_time = time.time()
        for perception in perceptions:
            bytes_data = perception.to_bytes()
            reconstructed = Perception.from_bytes(bytes_data)
        end_time = time.time()
        
        binary_duration = end_time - start_time
        
        # Binary debe ser más rápido que JSON
        self.assertLess(binary_duration, json_duration)
        
        print(f"JSON serialization: {json_duration:.3f}s")
        print(f"Binary serialization: {binary_duration:.3f}s")
    
    def test_similarity_calculation_performance(self):
        """Test rendimiento de cálculo de similitud"""
        import time
        
        # Crear conjunto de percepciones
        perceptions = [
            self.test_factory.create_sample_perception(f"sensor_{i}")
            for i in range(50)
        ]
        
        base_perception = perceptions[0]
        
        start_time = time.time()
        similarities = []
        
        for perception in perceptions[1:]:
            similarity = base_perception.similarity_score(perception)
            similarities.append(similarity)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Debe calcular 49 similitudes en menos de 0.1 segundos
        self.assertLess(duration, 0.1, "Similarity calculation too slow")
        self.assertEqual(len(similarities), 49)
        
        # Verificar que todas las similitudes están en rango válido
        for sim in similarities:
            self.assertTrue(0.0 <= sim <= 1.0)

# ====================================================================
# 11. TESTS DE ESTRÉS
# ====================================================================

class TestStress(TestAgentBase):
    """Tests de estrés y límites del sistema"""
    
    def test_large_data_handling(self):
        """Test manejo de datos grandes"""
        # Crear percepción con datos grandes
        large_data = {
            'readings': list(range(10000)),
            'metadata': {f'field_{i}': f'value_{i}' for i in range(1000)},
            'raw_data': 'x' * 100000  # 100KB de datos
        }
        
        perception = Perception(
            source="large_data_sensor",
            data=large_data,
            timestamp=datetime.now(),
            confidence=0.8
        )
        
        # Debe poder serializar y deserializar datos grandes
        json_str = perception.to_json()
        reconstructed = Perception.from_json(json_str)
        
        self.assertEqual(len(reconstructed.data['readings']), 10000)
        self.assertEqual(len(reconstructed.data['raw_data']), 100000)
    
    def test_concurrent_creation(self):
        """Test creación concurrente de objetos"""
        import threading
        import time
        
        results = []
        errors = []
        
        def create_perceptions():
            try:
                local_perceptions = []
                for i in range(100):
                    perception = self.test_factory.create_sample_perception(f"thread_sensor_{threading.current_thread().ident}_{i}")
                    local_perceptions.append(perception)
                results.extend(local_perceptions)
            except Exception as e:
                errors.append(e)
        
        # Crear múltiples threads
        threads = []
        for i in range(10):
            thread = threading.Thread(target=create_perceptions)
            threads.append(thread)
        
        # Ejecutar todos los threads
        start_time = time.time()
        for thread in threads:
            thread.start()
        
        for thread in threads:
            thread.join()
        
        end_time = time.time()
        
        # Verificar resultados
        self.assertEqual(len(errors), 0, f"Errors during concurrent creation: {errors}")
        self.assertEqual(len(results), 1000)  # 10 threads * 100 percepciones
        
        # Verificar que todos los IDs son únicos
        perception_ids = [p.perception_id for p in results]
        self.assertEqual(len(set(perception_ids)), len(perception_ids))
        
        print(f"Concurrent creation completed in {end_time - start_time:.3f}s")
    
    def test_memory_usage(self):
        """Test uso de memoria"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Crear muchos objetos
        perceptions = []
        actions = []
        decisions = []
        
        for i in range(1000):
            perception = self.test_factory.create_sample_perception(f"mem_test_{i}")
            action = self.test_factory.create_sample_action(f"mem_action_{i}")
            decision = self.test_factory.create_sample_decision()
            
            perceptions.append(perception)
            actions.append(action)
            decisions.append(decision)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        print(f"Memory usage increased by {memory_increase:.2f} MB for 3000 objects")
        
        # Verificar que el aumento de memoria es razonable (menos de 100MB para 3000 objetos)
        self.assertLess(memory_increase, 100, "Memory usage too high")

# ====================================================================
# 12. RUNNER DE TESTS Y BENCHMARKS
# ====================================================================

class TestSuite:
    """Suite completa de tests para el sistema de agentes"""
    
    def __init__(self):
        self.test_loader = unittest.TestLoader()
        self.test_runner = unittest.TextTestRunner(verbosity=2)
    
    def run_all_tests(self):
        """Ejecuta todos los tests"""
        print("=" * 70)
        print("EJECUTANDO SUITE COMPLETA DE TESTS PARA AGENTES DE IA")
        print("=" * 70)
        
        # Cargar todas las clases de test
        test_classes = [
            TestPerception,
            TestAction,
            TestDecision,
            TestAgentStateTransitions,
            TestIntegration,
            TestPerformance,
            TestStress
        ]
        
        suite = unittest.TestSuite()
        
        for test_class in test_classes:
            tests = self.test_loader.loadTestsFromTestCase(test_class)
            suite.addTests(tests)
        
        # Ejecutar tests
        result = self.test_runner.run(suite)
        
        # Resumen
        print("\n" + "=" * 70)
        print("RESUMEN DE TESTS")
        print("=" * 70)
        print(f"Tests ejecutados: {result.testsRun}")
        print(f"Errores: {len(result.errors)}")
        print(f"Fallos: {len(result.failures)}")
        print(f"Éxito: {result.wasSuccessful()}")
        
        if result.errors:
            print("\nERRORES:")
            for test, error in result.errors:
                print(f"- {test}: {error}")
        
        if result.failures:
            print("\nFALLOS:")
            for test, failure in result.failures:
                print(f"- {test}: {failure}")
        
        return result.wasSuccessful()
    
    def run_performance_benchmark(self):
        """Ejecuta benchmark de rendimiento"""
        print("\n" + "=" * 70)
        print("BENCHMARK DE RENDIMIENTO")
        print("=" * 70)
        
        import time
        import statistics
        
        factory = TestDataFactory()
        
        # Benchmark creación de percepciones
        times = []
        for _ in range(100):
            start = time.time()
            perception = factory.create_sample_perception()
            end = time.time()
            times.append((end - start) * 1000)  # en ms
        
        print(f"Creación de Perception:")
        print(f"  Promedio: {statistics.mean(times):.3f} ms")
        print(f"  Mediana: {statistics.median(times):.3f} ms")
        print(f"  Mínimo: {min(times):.3f} ms")
        print(f"  Máximo: {max(times):.3f} ms")
        
        # Benchmark serialización JSON
        perception = factory.create_sample_perception()
        times = []
        for _ in range(100):
            start = time.time()
            json_str = perception.to_json()
            end = time.time()
            times.append((end - start) * 1000)
        
        print(f"\nSerialización JSON:")
        print(f"  Promedio: {statistics.mean(times):.3f} ms")
        print(f"  Mediana: {statistics.median(times):.3f} ms")
        
        # Benchmark deserialización JSON
        json_str = perception.to_json()
        times = []
        for _ in range(100):
            start = time.time()
            reconstructed = Perception.from_json(json_str)
            end = time.time()
            times.append((end - start) * 1000)
        
        print(f"\nDeserialización JSON:")
        print(f"  Promedio: {statistics.mean(times):.3f} ms")
        print(f"  Mediana: {statistics.median(times):.3f} ms")
        
        # Benchmark cálculo de similitud
        perception1 = factory.create_sample_perception()
        perception2 = factory.create_sample_perception()
        times = []
        for _ in range(100):
            start = time.time()
            similarity = perception1.similarity_score(perception2)
            end = time.time()
            times.append((end - start) * 1000)
        
        print(f"\nCálculo de similitud:")
        print(f"  Promedio: {statistics.mean(times):.3f} ms")
        print(f"  Mediana: {statistics.median(times):.3f} ms")

# ====================================================================
# 13. UTILIDADES DE DEBUGGING Y MONITORING
# ====================================================================

class AgentDebugger:
    """Utilidades para debugging de agentes"""
    
    @staticmethod
    def analyze_perception(perception: Perception) -> Dict[str, Any]:
        """Analiza una percepción para debugging"""
        analysis = {
            'basic_info': {
                'id': perception.perception_id,
                'source': perception.source,
                'type': perception.metadata.get('type'),
                'confidence': perception.confidence,
                'age_seconds': (datetime.now() - perception.timestamp).total_seconds()
            },
            'data_analysis': {
                'data_size': len(str(perception.data)),
                'field_count': len(perception.data),
                'has_numeric': any(isinstance(v, (int, float)) for v in perception.data.values()),
                'complexity': perception._calculate_data_complexity()
            },
            'validation_status': {
                'is_expired': perception.is_expired(),
                'is_stale': perception.is_stale(),
                'hash': perception.calculate_hash()
            },
            'metadata_info': {
                'metadata_fields': list(perception.metadata.keys()),
                'processing_timestamp': perception.metadata.get('processing_timestamp'),
                'hash_match': perception.metadata.get('hash') == perception.calculate_hash()
            }
        }
        
        return analysis
    
    @staticmethod
    def analyze_action(action: Action) -> Dict[str, Any]:
        """Analiza una acción para debugging"""
        analysis = {
            'basic_info': {
                'id': action.action_id,
                'type': action.action_type,
                'priority': action.priority,
                'category': action.metadata.get('category')
            },
            'timing_info': {
                'is_due': action.is_due(),
                'is_expired': action.is_expired(),
                'estimated_duration': action.metadata.get('estimated_duration'),
                'timeout': action.timeout
            },
            'resource_info': {
                'cpu_intensive': action.resources_required.get('cpu_intensive'),
                'memory_mb': action.resources_required.get('memory_mb'),
                'network_required': action.resources_required.get('network_required'),
                'disk_space_mb': action.resources_required.get('disk_space_mb')
            },
            'parameters_info': {
                'param_count': len(action.parameters),
                'param_keys': list(action.parameters.keys()),
                'has_complex_params': any(isinstance(v, (dict, list)) for v in action.parameters.values())
            }
        }
        
        return analysis
    
    @staticmethod
    def analyze_decision(decision: Decision) -> Dict[str, Any]:
        """Analiza una decisión para debugging"""
        analysis = {
            'basic_info': {
                'id': decision.decision_id,
                'status': decision.execution_status,
                'confidence': decision.confidence,
                'options_count': len(decision.options),
                'selected_type': decision.selected_action.action_type
            },
            'quality_metrics': decision.calculate_decision_quality(),
            'context_analysis': {
                'context_size': len(decision.context),
                'context_complexity': decision.metadata.get('context_complexity'),
                'context_keys': list(decision.context.keys())
            },
            'alternative_analysis': {
                'rankings_available': len(decision.alternative_rankings) > 0,
                'top_alternative': decision.alternative_rankings[0] if decision.alternative_rankings else None,
                'score_spread': (
                    max(score for _, score in decision.alternative_rankings) - 
                    min(score for _, score in decision.alternative_rankings)
                ) if decision.alternative_rankings else 0
            },
            'execution_info': {
                'has_outcome': decision.outcome is not None,
                'status_history': len(decision.metadata.get('status_history', [])),
                'decision_age_seconds': (datetime.now() - decision.decision_time).total_seconds()
            }
        }
        
        return analysis
    
    @staticmethod
    def create_debug_report(perceptions: List[Perception], actions: List[Action], decisions: List[Decision]) -> str:
        """Crea reporte completo de debugging"""
        report = []
        report.append("=" * 80)
        report.append("REPORTE DE DEBUGGING DEL AGENTE")
        report.append("=" * 80)
        report.append(f"Timestamp: {datetime.now().isoformat()}")
        report.append("")
        
        # Análisis de percepciones
        report.append("PERCEPCIONES:")
        report.append(f"Total: {len(perceptions)}")
        if perceptions:
            expired_count = sum(1 for p in perceptions if p.is_expired())
            stale_count = sum(1 for p in perceptions if p.is_stale())
            avg_confidence = sum(p.confidence for p in perceptions) / len(perceptions)
            
            report.append(f"Expiradas: {expired_count}")
            report.append(f"Obsoletas: {stale_count}")
            report.append(f"Confianza promedio: {avg_confidence:.3f}")
            
            # Top percepciones por confianza
            top_perceptions = sorted(perceptions, key=lambda p: p.confidence, reverse=True)[:5]
            report.append("Top 5 por confianza:")
            for i, p in enumerate(top_perceptions, 1):
                report.append(f"  {i}. {p.source}: {p.confidence:.3f}")
        
        report.append("")
        
        # Análisis de acciones
        report.append("ACCIONES:")
        report.append(f"Total: {len(actions)}")
        if actions:
            due_count = sum(1 for a in actions if a.is_due())
            expired_count = sum(1 for a in actions if a.is_expired())
            priority_dist = {}
            for action in actions:
                priority_dist[action.priority] = priority_dist.get(action.priority, 0) + 1
            
            report.append(f"Listas para ejecutar: {due_count}")
            report.append(f"Expiradas: {expired_count}")
            report.append("Distribución por prioridad:")
            for priority in sorted(priority_dist.keys()):
                report.append(f"  Prioridad {priority}: {priority_dist[priority]}")
        
        report.append("")
        
        # Análisis de decisiones
        report.append("DECISIONES:")
        report.append(f"Total: {len(decisions)}")
        if decisions:
            status_dist = {}
            avg_confidence = sum(d.confidence for d in decisions) / len(decisions)
            
            for decision in decisions:
                status = decision.execution_status
                status_dist[status] = status_dist.get(status, 0) + 1
            
            report.append(f"Confianza promedio: {avg_confidence:.3f}")
            report.append("Distribución por estado:")
            for status in sorted(status_dist.keys()):
                report.append(f"  {status}: {status_dist[status]}")
            
            # Decisiones con outcome
            with_outcome = [d for d in decisions if d.outcome is not None]
            if with_outcome:
                quality_scores = [d.calculate_decision_quality().get('overall_quality', 0) for d in with_outcome]
                avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
                report.append(f"Calidad promedio (con outcome): {avg_quality:.3f}")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)

# ====================================================================
# 14. EJEMPLO DE USO Y DEMO
# ====================================================================

def demo_agent_base_system():
    """Demostración del sistema base de agentes"""
    print("=" * 70)
    print("DEMOSTRACIÓN DEL SISTEMA BASE DE AGENTES")
    print("=" * 70)
    
    factory = TestDataFactory()
    debugger = AgentDebugger()
    
    # 1. Crear percepciones de ejemplo
    print("\n1. Creando percepciones de ejemplo...")
    perceptions = []
    
    # Percepción de sensor de temperatura
    temp_perception = Perception(
        source="temperature_sensor_01",
        data={
            'value': 28.5,
            'unit': 'celsius',
            'sensor_id': 'TEMP_001',
            'location': 'construction_site_A'
        },
        timestamp=datetime.now(),
        confidence=0.95,
        metadata={
            'type': PerceptionType.SENSOR_READING.value,
            'calibration_date': '2024-01-15',
            'maintenance_status': 'good'
        }
    )
    perceptions.append(temp_perception)
    
    # Percepción de API de clima
    weather_perception = Perception(
        source="weather_api",
        data={
            'status': 200,
            'response': {
                'temperature': 30.0,
                'humidity': 65,
                'wind_speed': 15,
                'conditions': 'partly_cloudy'
            }
        },
        timestamp=datetime.now(),
        confidence=0.88,
        metadata={
            'type': PerceptionType.API_RESPONSE.value,
            'api_version': 'v2.1',
            'response_time': 250
        }
    )
    perceptions.append(weather_perception)
    
    print(f"Creadas {len(perceptions)} percepciones")
    
    # 2. Crear acciones basadas en percepciones
    print("\n2. Generando acciones basadas en percepciones...")
    actions = []
    
    # Acción basada en temperatura alta
    if temp_perception.data['value'] > 25:
        cooling_action = Action(
            action_type="activate_cooling",
            parameters={
                'target_temperature': 24.0,
                'current_temperature': temp_perception.data['value'],
                'location': temp_perception.data['location']
            },
            priority=Priority.HIGH.value,
            expected_outcome="Temperature reduced to safe working levels"
        )
        actions.append(cooling_action)
    
    # Acción de monitoreo continuo
    monitoring_action = Action(
        action_type="continuous_monitoring",
        parameters={
            'sensors': ['TEMP_001'],
            'interval_minutes': 5,
            'alert_threshold': 30.0
        },
        priority=Priority.MEDIUM.value,
        expected_outcome="Continuous temperature monitoring established"
    )
    actions.append(monitoring_action)
    
    # Acción de notificación
    if weather_perception.data['response']['wind_speed'] > 10:
        alert_action = Action(
            action_type="send_wind_alert",
            parameters={
                'wind_speed': weather_perception.data['response']['wind_speed'],
                'alert_level': 'moderate',
                'affected_areas': ['construction_site_A']
            },
            priority=Priority.HIGH.value,
            expected_outcome="Work crews notified of wind conditions"
        )
        actions.append(alert_action)
    
    print(f"Generadas {len(actions)} acciones")
    
    # 3. Crear decisión basada en acciones
    print("\n3. Tomando decisión...")
    
    # Simular proceso de decisión (normalmente hecho por motor de decisión)
    context = {
        'current_conditions': {
            'temperature': temp_perception.data['value'],
            'wind_speed': weather_perception.data['response']['wind_speed'],
            'humidity': weather_perception.data['response']['humidity']
        },
        'safety_requirements': {
            'max_temperature': 30.0,
            'max_wind_speed': 25.0
        },
        'operational_status': 'active',
        'worker_count': 15,
        'urgency_level': 'normal'
    }
    
    # Seleccionar acción de mayor prioridad
    selected_action = min(actions, key=lambda a: a.priority)
    
    decision = Decision(
        decision_id=str(uuid.uuid4()),
        context=context,
        options=actions,
        selected_action=selected_action,
        reasoning=f"Selected '{selected_action.action_type}' due to highest priority ({selected_action.priority}) and current environmental conditions requiring immediate response",
        confidence=0.87
    )
    
    print(f"Decisión tomada: {decision.selected_action.action_type}")
    print(f"Confianza: {decision.confidence:.1%}")
    
    # 4. Análisis de debugging
    print("\n4. Análisis de debugging...")
    
    for i, perception in enumerate(perceptions):
        analysis = debugger.analyze_perception(perception)
        print(f"\nPercepción {i+1} ({perception.source}):")
        print(f"  Tipo: {analysis['basic_info']['type']}")
        print(f"  Confianza: {analysis['basic_info']['confidence']}")
        print(f"  Edad: {analysis['basic_info']['age_seconds']:.1f}s")
        print(f"  Complejidad: {analysis['data_analysis']['complexity']:.1f}")
    
    for i, action in enumerate(actions):
        analysis = debugger.analyze_action(action)
        print(f"\nAcción {i+1} ({action.action_type}):")
        print(f"  Prioridad: {analysis['basic_info']['priority']}")
        print(f"  Categoría: {analysis['basic_info']['category']}")
        print(f"  Lista para ejecutar: {analysis['timing_info']['is_due']}")
        print(f"  Requiere CPU intensivo: {analysis['resource_info']['cpu_intensive']}")
    
    decision_analysis = debugger.analyze_decision(decision)
    print(f"\nDecisión:")
    print(f"  Estado: {decision_analysis['basic_info']['status']}")
    print(f"  Opciones consideradas: {decision_analysis['basic_info']['options_count']}")
    print(f"  Complejidad del contexto: {decision_analysis['context_analysis']['context_complexity']:.1f}")
    
    # 5. Serialización y persistencia
    print("\n5. Demostrando serialización...")
    
    # JSON
    decision_json = decision.to_json()
    print(f"Decisión serializada a JSON ({len(decision_json)} caracteres)")
    
    # Reconstruir desde JSON
    reconstructed_decision = Decision.from_json(decision_json)
    print(f"Decisión reconstruida exitosamente")
    print(f"ID coincide: {decision.decision_id == reconstructed_decision.decision_id}")
    
    # Binary
    decision_bytes = decision.to_bytes()
    print(f"Decisión serializada a binario ({len(decision_bytes)} bytes)")
    
    reconstructed_binary = Decision.from_bytes(decision_bytes)
    print(f"Reconstrucción binaria exitosa: {decision.decision_id == reconstructed_binary.decision_id}")
    
    # 6. Reporte final
    print("\n6. Reporte de debugging completo:")
    debug_report = debugger.create_debug_report(perceptions, actions, [decision])
    print(debug_report)
    
    print("\n" + "=" * 70)
    print("DEMOSTRACIÓN COMPLETADA")
    print("=" * 70)

# ====================================================================
# 15. MAIN Y ENTRY POINT
# ====================================================================

def main():
    """Función principal para ejecutar tests y demos"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Sistema Base de Agentes IA con Testing")
    parser.add_argument('--tests', action='store_true', help='Ejecutar todos los tests')
    parser.add_argument('--benchmark', action='store_true', help='Ejecutar benchmark de rendimiento')
    parser.add_argument('--demo', action='store_true', help='Ejecutar demostración')
    parser.add_argument('--all', action='store_true', help='Ejecutar tests, benchmark y demo')
    
    args = parser.parse_args()
    
    if not any([args.tests, args.benchmark, args.demo, args.all]):
        args.demo = True  # Por defecto ejecutar demo
    
    success = True
    
    if args.tests or args.all:
        test_suite = TestSuite()
        success = test_suite.run_all_tests() and success
    
    if args.benchmark or args.all:
        test_suite = TestSuite()
        test_suite.run_performance_benchmark()
    
    if args.demo or args.all:
        demo_agent_base_system()
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())